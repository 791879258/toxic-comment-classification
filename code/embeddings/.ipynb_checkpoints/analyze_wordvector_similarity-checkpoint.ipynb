{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Model\t| Embeddings | Private | Public | Local |\n",
    "|:------ |:---------- | ------- | ------ | ----- |\n",
    "| CapsuleNet\t| fasttext\t| 0.9855\t| 0.9867\t| 0.9896|\n",
    "| CapsuleNet\t| glove\t| 0.9860 \t| 0.9859\t| 0.9899|\n",
    "| CapsuleNet\t| lexvec\t| 0.9855\t| 0.9858\t| 0.9898|\n",
    "| CapsuleNet\t| toxic\t| 0.9859\t| 0.9863\t| 0.9901|\n",
    "\n",
    "| Model\t| Embeddings | Private | Public | Local |\n",
    "|:------ |:---------- | ------- | ------ | ----- |\n",
    "| RNN Version 2\t| fasttext\t| 0.9856\t| 0.9864\t| 0.9904|\n",
    "| RNN Version 2\t| glove\t| 0.9858 \t| 0.9863\t| 0.9902|\n",
    "| RNN Version 2\t| lexvec\t| 0.9857\t| 0.9859\t| 0.9902|\n",
    "| RNN Version 2\t| toxic\t| 0.9851\t| 0.9855\t| 0.9906|\n",
    "\n",
    "| Model\t| Embeddings | Private | Public | Local |\n",
    "|:------ |:---------- | ------- | ------ | ----- |\n",
    "| RNN Version 1\t| fasttext\t| 0.9853\t| 0.9859\t| 0.9898|\n",
    "| RNN Version 1\t| glove\t| 0.9855\t| 0.9861\t| 0.9901|\n",
    "| RNN Version 1\t| lexvec\t| 0.9854\t| 0.9857\t| 0.9897|\n",
    "| RNN Version 1\t| toxic\t| 0.9856 | 0.9861\t| 0.9903|\n",
    "\n",
    "| Model\t| Embeddings | Private | Public | Local |\n",
    "|:------ |:---------- | ------- | ------ | ----- |\n",
    "| 2 Layer CNN\t| fasttext\t| 0.9826\t| 0.9835\t| 0.9886|\n",
    "| 2 Layer CNN\t| glove \t| 0.9827\t| 0.9828\t| 0.9883|\n",
    "| 2 Layer CNN\t| lexvec\t| 0.9824\t| 0.9831\t| 0.9880|\n",
    "| 2 Layer CNN\t| toxic\t| 0.9806\t| 0.9789\t| 0.9880|\n",
    "| SVM with NB features\t| NA\t| 0.9813\t| 0.9813\t| 0.9863|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "For each missing word `w` in `missing_words`, find the most similar word `most_similar_word` in `shared_words`:\n",
    "\n",
    "```\n",
    "local = {local_words: local_vectors}\n",
    "external = {external_words: external_vectors}\n",
    "shared_words = intersect(local_words, external_words)\n",
    "missing_words = setdiff(local_words, external_words)\n",
    "reference_matrix = array(local[w] for w in shared_words).T\n",
    "\n",
    "for w in missing_words:\n",
    "     similarity = local[w] * reference_matrix\n",
    "     most_similar_word = shared_words[argmax(similarity)]\n",
    "     external[w] = external_vectors[most_similar_word]\n",
    "\n",
    "return {w: external[w] for w in local_words}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../../input/tmp/fasttext-local.txt\n",
      "Loading ../../input/tmp/glove.twitter.27B.200d.txt\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "def read_embedding(file_name):\n",
    "  print(f'Loading {file_name}')\n",
    "  f = open(file_name, encoding='utf-8')\n",
    "  word_list, word_vectors = [],{}\n",
    "  for line in f:\n",
    "    split_line = line.split()\n",
    "    w = split_line[0]\n",
    "    v = np.array([float(val) for val in split_line[1:]])        \n",
    "\n",
    "    word_list.append(w)\n",
    "    word_vectors[w] = v        \n",
    "  return np.array(word_list), word_vectors\n",
    "  \n",
    "local_words, local_vectors = read_embedding('../../input/tmp/fasttext-local.txt')\n",
    "external_words, external_vectors = read_embedding('../../input/tmp/glove.twitter.27B.200d.txt')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find missing and shared words\n",
    "missing_words = np.setdiff1d(local_words, external_words)\n",
    "shared_words = np.intersect1d(local_words, external_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create reference matrix\n",
    "reference_matrix = np.array([local_vectors[w] for w in shared_words])\n",
    "reference_matrix = normalize(reference_matrix).T # word vectors are columns\n",
    "\n",
    "# create lookup matrix\n",
    "lookup_matrix = np.array([local_vectors[w] for w in missing_words])\n",
    "lookup_matrix = normalize(lookup_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: succesors    most similar: succes\n",
      "missing: ratul        most similar: satul\n",
      "missing: donorsthat   most similar: donors\n",
      "missing: pollypocke   most similar: lollypops\n",
      "missing: controvert   most similar: contentious\n",
      "missing: actualyy     most similar: actualy\n",
      "missing: cjaed        most similar: cja\n",
      "missing: labadii      most similar: hadii\n",
      "missing: dreun        most similar: sik\n",
      "missing: vadabalija   most similar: dijalankan\n"
     ]
    }
   ],
   "source": [
    "# find words similar to random missing words\n",
    "for w in np.random.choice(missing_words, 10):\n",
    "  similarity = np.matmul(local_vectors[w], reference_matrix)\n",
    "  similar_word = shared_words[np.argmax(similarity)]\n",
    "  print('missing: {0: <10}   most similar: {1}'.format(w[:10], similar_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through missing words\n",
    "* uses least memory on CPU\n",
    "* slowest implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_loop():\n",
    "  for w in missing_words:\n",
    "    similarity = np.matmul(local_vectors[w], reference_matrix)\n",
    "    similar_word = shared_words[np.argmax(similarity)]\n",
    "    external_vectors[w] = external_vectors[similar_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124.9556284940045"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(fill_missing_loop, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized computation\n",
    "* needs more memory (more than 16 GB)\n",
    "* almost 5x faster than looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_vectorized():\n",
    "  similarity = np.matmul(lookup_matrix, reference_matrix)\n",
    "  similar_words = shared_words[np.argmax(similarity, axis=1)]\n",
    "  for m,s in zip(missing_words, similar_words):\n",
    "    external_vectors[m] = external_vectors[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205.92537692499172"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(fill_missing_vectorized, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation on GPU\n",
    "* requires pytorch\n",
    "* need to optimize chunk size\n",
    "* fastest implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_missing_gpu_chunks():\n",
    "  \n",
    "  # setup\n",
    "  chunk_size = 500\n",
    "  n_lookups = lookup_matrix.shape[0]\n",
    "  n_chunks = n_lookups//chunk_size+1\n",
    "\n",
    "  # convert to numpy array to torch tensors\n",
    "  dtype = torch.cuda.FloatTensor  \n",
    "  def np2tc(x): return torch.from_numpy(x).type(dtype)\n",
    "  reference_matrix_gpu = np2tc(reference_matrix)\n",
    "  \n",
    "  # iterate through chunks\n",
    "  for i in range(n_chunks):\n",
    "    chunk_indexs = slice(chunk_size*i, min(chunk_size*(i+1), n_lookups))\n",
    "    similarity = torch.mm(np2tc(lookup_matrix[chunk_indexs]), reference_matrix_gpu)\n",
    "    _, similar_indexs = torch.max(similarity, 1)\n",
    "    similar_words = shared_words[np.array(similar_indexs)]\n",
    "    for m,s in zip(missing_words[chunk_indexs], similar_words):\n",
    "      external_vectors[m] = external_vectors[s] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.229991279993556"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeit.timeit(fill_missing_gpu_chunks, number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "chunk_size  GPU RAM  duration (s)\n",
    "100         2.0 GB   68.6401116699999\n",
    "500         2.7 GB   52.75159321799583\n",
    "1000        3.5 GB   53.69221766899864\n",
    "2500        6.1 GB   54.229991279993556\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_embedding(save_name):\n",
    "  fwrite = open(save_name, 'w')\n",
    "  for word, vec in external_vectors.items():\n",
    "    fwrite.write(word + ' ' + ' '.join(vec.astype(str)) + '\\n')\n",
    "  fwrite.close()      \n",
    "  \n",
    "write_embedding('../../input/tmp/output-embedding.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
